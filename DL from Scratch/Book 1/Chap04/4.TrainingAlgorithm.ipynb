{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.TrainingAlgorithm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1phNCwCnw3AdgabqeIVGnL7IJ0CjwA_gq",
      "authorship_tag": "ABX9TyMPBp2B43JCRnTKc9f8Q46A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cow-coding/ML-DL-Study/blob/master/DL%20from%20Scratch/Book%201/Chap04/4.TrainingAlgorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hexQrR8Ppftx"
      },
      "source": [
        "# Library Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26kN10YgphTY",
        "outputId": "8588605a-a294-45f4-9ddb-0bf5108fd63c"
      },
      "source": [
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset.mnist import load_mnist\n",
        "from common.functions import *                  # 책코드\n",
        "from common.gradient import numerical_gradient  # 책코드"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMzK2EXqposG"
      },
      "source": [
        "# Training Algorithm\n",
        "\n",
        "이제 본격적으로 학습 알고리즘을 설계해보자.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Minibatch\n",
        "\n",
        "미니배치는 훈련 데이터 중 일부를 무작위로 추출한다.  \n",
        "미니배치의 손실 함수 값을 줄이는 것을 목표로 한다.\n",
        "\n",
        "## 2. Calculate Gradient\n",
        "\n",
        "미니배치의 손실 함수 값을 줄이기 위해 가중치의 매개변수 기울기를 구한다.  \n",
        "구한 기울기로 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
        "\n",
        "## 3. Update the weight parameter\n",
        "\n",
        "가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.\n",
        "\n",
        "## 4. Repeat\n",
        "\n",
        "1 ~ 3단계를 반복\n",
        "\n",
        "---\n",
        "\n",
        "이때 데이터를 미니배치로 선정하므로 **확률적 경사 하강법(Stochastic Gradient Descent, SGD)**이라고 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0gj5Ea1rCwi"
      },
      "source": [
        "## 2 Layer Neural Network Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUvMjMQvpcIY"
      },
      "source": [
        "class TwoLayerNet:\n",
        "  def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
        "    # initialize the weight\n",
        "    self.params = {}\n",
        "    \n",
        "    self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
        "    self.params['b1'] = np.zeros(hidden_size)\n",
        "    \n",
        "    self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
        "    self.params['b2'] = np.zeros(output_size)\n",
        "\n",
        "  def predict(self, x):\n",
        "    W1, W2 = self.params['W1'], self.params['W2']\n",
        "    b1, b2 = self.params['b1'], self.params['b2']\n",
        "\n",
        "    a1 = np.dot(x, W1) + b1\n",
        "    z1 = sigmoid(a1)\n",
        "\n",
        "    a2 = np.dot(z1, W2) + b2\n",
        "    y = softmax(a2)\n",
        "\n",
        "    return y\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    y = self.predict(x)\n",
        "\n",
        "    return cross_entropy_error(y, t)\n",
        "\n",
        "  def accuracy(self, x, t):\n",
        "    y = self.predict(x)\n",
        "    y = np.argmax(y, axis=1)\n",
        "    t = np.argmax(t, axis=1)\n",
        "\n",
        "    accuracy = np.sum(y == t) / float(x.shape[0])\n",
        "    return accuracy\n",
        "\n",
        "  def numerical_gradient(self, x, t):\n",
        "    loss_W = lambda W: self.loss(x, t)\n",
        "\n",
        "    grads = {}\n",
        "    grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
        "    grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
        "    grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
        "    grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
        "\n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DuV5tt80VEX"
      },
      "source": [
        "### TwoLayerNet class usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7KEgtIGyMi9",
        "outputId": "4811bbc7-c444-4d93-ee02-e8c3ce54e3ea"
      },
      "source": [
        "net = TwoLayerNet(input_size=784, hidden_size=100, output_size=10)\n",
        "\n",
        "print(net.params['W1'].shape)\n",
        "print(net.params['b1'].shape)\n",
        "print(net.params['W2'].shape)\n",
        "print(net.params['b2'].shape)\n",
        "\n",
        "x = np.random.randn(100, 784)         # dummy input data\n",
        "t = np.random.randn(100, 10)          # dummy answer label\n",
        "\n",
        "grads = net.numerical_gradient(x, t)  # calculate gradient"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(784, 100)\n",
            "(100,)\n",
            "(100, 10)\n",
            "(10,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BA1Ak1B0aFH"
      },
      "source": [
        "## Implementation minibatch training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44MLterAy2K7"
      },
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "train_loss_list = []\n",
        "\n",
        "# hyperparameter\n",
        "iters_num = 10000               # repeat num\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100                # mini-batch size\n",
        "learning_rate = 0.1\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  # get mini-batch\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  # calculate gradient\n",
        "  grad = network.numerical_gradient(x_batch, t_batch)\n",
        "\n",
        "\n",
        "  # Update parameter\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "    network.params[key] -= learning_rate + grad[key]\n",
        "\n",
        "  # save training log\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KwjQAE_2KJX"
      },
      "source": [
        "## minibatch training by epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Zf3TJPO1tdo",
        "outputId": "6e80f64f-b38d-4c09-9656-9aaf75633d44"
      },
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
        "\n",
        "# hyperparameter\n",
        "iters_num = 10000               # repeat num\n",
        "train_size = x_train.shape[0]\n",
        "batch_size = 100                # mini-batch size\n",
        "learning_rate = 0.1\n",
        "\n",
        "train_loss_list = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "\n",
        "# repeat num by 1epoch\n",
        "iter_per_epoch = max(train_size / batch_size, 1)\n",
        "\n",
        "for i in range(iters_num):\n",
        "  # get mini-batch\n",
        "  batch_mask = np.random.choice(train_size, batch_size)\n",
        "  x_batch = x_train[batch_mask]\n",
        "  t_batch = t_train[batch_mask]\n",
        "\n",
        "  # calculate gradient\n",
        "  grad = network.numerical_gradient(x_batch, t_batch)\n",
        "\n",
        "\n",
        "  # Update parameter\n",
        "  for key in ('W1', 'b1', 'W2', 'b2'):\n",
        "    network.params[key] -= learning_rate + grad[key]\n",
        "\n",
        "  # save training log\n",
        "  loss = network.loss(x_batch, t_batch)\n",
        "  train_loss_list.append(loss)\n",
        "\n",
        "  # calculate accuracy by 1epoch\n",
        "  if i % iter_per_epoch == 0:\n",
        "    train_acc = network.accuracy(x_train, t_train)\n",
        "    test_acc = network.accuracy(x_test, t_test)\n",
        "    \n",
        "    train_acc_list.append(train_acc)\n",
        "    test_acc_list.append(test_acc)\n",
        "\n",
        "    print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train acc, test acc | 0.09863333333333334, 0.0958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/common/functions.py:14: RuntimeWarning: overflow encountered in exp\n",
            "  return 1 / (1 + np.exp(-x))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}