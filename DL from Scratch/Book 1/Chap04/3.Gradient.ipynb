{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.Gradient.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1n0OAUKEI-xGD8dlnphRN5CiVtUBcYcDR",
      "authorship_tag": "ABX9TyNBKtaIvuAFO7/PETWy9eUX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cow-coding/ML-DL-Study/blob/master/DL%20from%20Scratch/Book%201/Chap04/3.Gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGVN-EdeLlag"
      },
      "source": [
        "# Library Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2Zsm42DLhUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecdb3f60-2732-42b6-b718-663a81194ccc"
      },
      "source": [
        "%cd /content/drive/MyDrive/Colab\\ Notebooks/\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from common.functions import softmax, cross_entropy_error\n",
        "from common.gradient import numerical_gradient"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfA87NWJN926"
      },
      "source": [
        "# Base function setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oB9ML_8N_jG"
      },
      "source": [
        "def function_2(x):\n",
        "  return np.sum(x**2)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mGcgIKNLw6A"
      },
      "source": [
        "# Gradient\n",
        "\n",
        "이전에 구현한 편미분은 각각의 변수를 따로 관리하는 방식으로 구현했다.  \n",
        "하지만 그렇게 구현하면 변수의 수만큼 함수를 만들어야한다.  \n",
        "따라서 동시에 편미분을 하는 방법이 필요하다.  \n",
        "이때 사용하는 방법이 벡터를 활용한 편미분 연산인데, 이를 **기울기(Gradient)**라고 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySfJGW0GMHNk"
      },
      "source": [
        "## Numerical Gradient Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1-0DPgWLsNg"
      },
      "source": [
        "def numerical_gradient_1d(f, x):\n",
        "  h = 1e-4  # 0.0001\n",
        "  grad = np.zeros_like(x) # x와 형상이 같은 배열 0으로 생성\n",
        "\n",
        "  for idx in range(x.size):\n",
        "    tmp_val = x[idx]  \n",
        "    \n",
        "    # f(x+h) 계산\n",
        "    x[idx] = tmp_val + h\n",
        "    fxh1 = f(x)\n",
        "\n",
        "    # f(x-h) 계산\n",
        "    x[idx] = tmp_val - h\n",
        "    fxh2 = f(x)\n",
        "\n",
        "    grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
        "    x[idx] = tmp_val  # 값 복원\n",
        "\n",
        "  return grad"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhKaAWFePRE-"
      },
      "source": [
        "### Example Numerical Gradient"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiFykGHuN4o1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1877a3f3-5cda-43d1-fa3c-524bf41834b5"
      },
      "source": [
        "print(numerical_gradient_1d(function_2, np.array([3.0, 4.0])))\n",
        "print(numerical_gradient_1d(function_2, np.array([0.0, 2.0])))\n",
        "print(numerical_gradient_1d(function_2, np.array([3.0, 0.0])))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[6. 8.]\n",
            "[0. 4.]\n",
            "[6. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67gQHPlCPVvq"
      },
      "source": [
        "# Gradient Method\n",
        "- Gradient Descent\n",
        "- Gradient Ascent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnKXCClfQS5F"
      },
      "source": [
        "$$\n",
        "x_{0} = x_{0} - \\eta \\frac{\\partial{f}}{\\partial{x_{0}}}\n",
        "$$  \n",
        "\n",
        "$$\n",
        "x_{1} = x_{1} - \\eta \\frac{\\partial{f}}{\\partial{x_{1}}}\n",
        "$$  \n",
        "\n",
        "$\\eta$ : 학습률(learning rate) - 갱신하는 양"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLbm_je4PbQP"
      },
      "source": [
        "일반적인 손실 함수는 매우 복잡핟. 따라서 매개변수 공간에서 어디가 최솟값인 공간인지 알기가 어렵다.  \n",
        "이럴때 기울기를 이용해 찾는 것이 **경사법(Gradient Method), 경사하강법(Gradient Descent)**이다.  \n",
        "여기서 주의할 점은 함수 값을 낮추는 방안을 제시하는 지표가 기울기라는 것이다.  \n",
        "결과적으로 기울기가 가리키는 곳이 극소(local minimum)인지 최소(global minimum)인지 구분할 수 없다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oknNd3hxRR-5"
      },
      "source": [
        "## Gradient Descent Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nkS0-4YO_6v"
      },
      "source": [
        "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
        "  x = init_x\n",
        "  x_history = [] # just graph making code\n",
        "\n",
        "  for i in range(step_num):\n",
        "    x_history.append(x.copy()) # just graph making code\n",
        "    grad = numerical_gradient_1d(f, x)\n",
        "    x -= lr * grad\n",
        "\n",
        "  return x, np.array(x_history)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSYaAqQ3RzLf"
      },
      "source": [
        "### Example of Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tt0YhGSqRg0M",
        "outputId": "e9309e28-0821-4027-e390-d1a86f7f187c"
      },
      "source": [
        "init_x = np.array([-3.0, 4.0])\n",
        "\n",
        "x, x_history = gradient_descent(function_2, init_x, 0.1, 100)\n",
        "print(x)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-6.11110793e-10  8.14814391e-10]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHy0DoG7Svm0"
      },
      "source": [
        "### Change of calculation result by GD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "hmJo01IGSYrL",
        "outputId": "70e69772-c512-4363-998d-94c03b513492"
      },
      "source": [
        "plt.plot( [-5, 5], [0,0], '--b')\n",
        "plt.plot( [0,0], [-5, 5], '--b')\n",
        "plt.plot(x_history[:,0], x_history[:,1], 'o')\n",
        "\n",
        "plt.xlim(-3.5, 3.5)\n",
        "plt.ylim(-4.5, 4.5)\n",
        "plt.xlabel(\"X0\")\n",
        "plt.ylabel(\"X1\")\n",
        "plt.show()"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAVdklEQVR4nO3dfZBddX3H8c/HFHHBOqlkK5AshhkhlRLc1B3kwVqEKAESQUEiU6KprRtALdEEShLAUZ6igGamFSZpobHAaDI8KZgIBMhQJ6JsYHkmyFhistqyqKkiO5XAt3+cuybZ53vv3vu75573a+bM2XvP3b0fMsv97u/xOCIEACieN6UOAABIgwIAAAVFAQCAgqIAAEBBUQAAoKD+JHWAckyaNCmmTp2aOgYA5MrmzZtfjojWgc/nqgBMnTpVXV1dqWMAe9i2LTu3taXNAQzH9tahns9VAQAa0bx52XnjxqQxgLIxBgAABUUBAICCogAAQEFRAACgoBgEBqq0aFHqBEBlKABAlebMSZ0AqEzyAmB7gqQuST0RMTtFhjsf69HV92zRL3b06cCJLbrgxGk6bcbkFFGQQ1u2ZOdp09LmAMqVvABIOl/Ss5LeluLN73ysR0tuf1J9r70uSerZ0acltz8pSRQBjMmCBdmZdQDIm6SDwLanSDpF0r+lynD1PVv++OHfr++113X1PVsSJQKA+kg9C2iFpAslvTHcC2x32u6y3dXb2zvuAX6xo6+s5wGgWSQrALZnS3opIjaP9LqIWBURHRHR0do6aC+jqh04saWs5wGgWaRsARwr6SO2X5T0HUnH27653iEuOHGaWvaasMdzLXtN0AUnMqIHoLklGwSOiCWSlkiS7eMkLY6Is+udo3+gl1lAqNTFF6dOAFSmEWYBJXfajMl84KNiM2emTgBUpiEKQERslLQxcQygIt3d2bm9PW0OoFwNUQCAPFu4MDuzDgB5k3oaKAAgEQoAABQUBQAACooCAAAFxSAwUKUrr0ydAKgMBQCo0jHHpE4AVIYuIKBKmzZlB5A3tACAKi1dmp1ZB4C8oQUAAAVFAQCAgqILKBHuQwwgNQpAAtyHGEAjoAAkMNJ9iCkA+bNiReoEQGUoAAlwH+LmwjbQyKuU9wR+i+2f2H7c9tO2v5wqS71xH+LmsmFDdgB5k3IW0P9JOj4i3iOpXdIs20clzFM33Ie4uVx+eXYAeZPynsAh6ZXSw71KR6TKU0/chxhAI0g6BmB7gqTNkt4l6ZsR8eOUeeqJ+xADSC3pQrCIeD0i2iVNkXSk7cMHvsZ2p+0u2129vb31DwkATaohVgJHxA5JD0qaNcS1VRHREREdra2t9Q8HAE0qWReQ7VZJr0XEDtstkj4k6aup8gCVWrkydQKgMinHAA6Q9K3SOMCbJK2NiLsT5gEqMo3JW8iplLOAnpA0I9X7A+Plrruy85w5aXMA5WIlMFCla6/NzhQA5E1DDAIDAOqPFkATYqtpAGNBAWgybDUNYKzoAmoyI201DQC7owXQZNhquv5uuil1AqAyFIAmc+DEFvUM8WHPVtO109aWOgFQGbqAmgxbTdffmjXZAeQNLYAmw1bT9Xf99dl57ty0OYByUQCaEFtNAxgLuoAAoKAoAABQUBQAACgoxgCAKt16a+oEQGUoAECVJk1KnQCoDAUAw2JTubFZvTo7z5+fMgVQvmRjALbbbD9o+xnbT9s+P1UWDNa/qVzPjj6Fdm0qd+djPamjNZzVq3cVASBPUg4C75S0KCIOk3SUpM/aPixhHuyGTeWA5pesAETELyPi0dLXv5P0rCT6FxoEm8oBza8hpoHanqrs/sA/HuJap+0u2129vb31jlZYw20ex6ZyQPNIXgBsv1XSbZIWRsRvB16PiFUR0RERHa2trfUPWFBsKgc0v6SzgGzvpezD/5aIuD1lFuyJTeXGbt261AmAyiQrALYt6QZJz0bE11PlwPDYVG5s9tkndQKgMim7gI6VNE/S8ba7S8fJCfMAFbnuuuwA8iZZCyAifijJqd4ftVWkRWRr12bn885LmwMoFyuBMe76F5H1ryPoX0QmqWmLAJBHyWcBofmwiAzIBwoAxh2LyIB8oABg3LGIDMgHCgDGXdEWkW3cmB1A3jAIjHHHIjIgHygAqIkiLSK75prsvHhx2hxAuSgASC7vawbuvjs7UwCQNxQAJMWaASAdBoGRFGsGgHQoAEiKNQNAOhQAJNUMawZaWrIDyBsKAJJqhjUD69dnB5A3DAIjKdYMAOlQAJDcWNcMNOp00csuy86XXJI2B1CupF1Atm+0/ZLtp1LmQOPrny7as6NPoV3TRe98rCd1NN1/f3YAeZN6DGC1pFmJMyAHmC4KjL+kBSAiHpL065QZkA9MFwXGX+oWwKhsd9rust3V29ubOg4SaYbpokCjafgCEBGrIqIjIjpaW1tTx0Eio00XvfOxHh27/AEdfNH3dezyB+o6NrDfftkB5A2zgJALI00XTb2f0G231fwtgJqgACA3hpsuOtIAcSNMEwUaVeppoN+W9CNJ02xvt/33KfMgn1IPEC9Zkh1A3iRtAUTEWSnfH83hwIkt6hniw/7AiS11WTz2ox+N648D6qbhB4GB0Qw3QPzBv2ht2MVjQCOgACD3TpsxWVd9bLomT2yRJU2e2KKrPjZdDz7Xy+IxYAQMAqMpDDVA/IU13UO+tmdHn45d/kDD7SkE1BsFAE1ruLEBS398fjymjE6ZUnFEICm6gNC0hhobsKQY8Lpqu4Vuvjk7gLyhAKBpDTU2MPDDv1/Pjr4kq4iBlOgCQlMbODZw7PIHhuwWkrTHTKH+7x2LhQuz84oVVUUF6o4WAAplqG6hgfpee10L13SPuTXQ3Z0dQN5QAFAoA7uFRtKzo08L13RrxlfupVsITYkuIBTO7t1CI3UJ9fvNq6/VdXM5oF5oAaDQxtIlJGXdQovWPk5LAE2FAoBC271LaDSvRwzZJXToodkB5I0jhpsY13g6Ojqiq6srdQw0qYH3FRjNvm+eoCs+Op1uITQ825sjomPg87QAgJL+1sDElr3G9Prf/yGbLfSXl/6AriHkEi0AYAh3PtajRWsf1+tj/f8jpEPesa/u++JxNc0FVGJcWwC2P1R9JMn2LNtbbL9g+6Lx+JnAeDhtxmRde+Z7xjRALEmy9NOXfq+pF32/tsGAcVRpF9AN1b6x7QmSvinpJEmHSTrL9mHV/lxgvJTbJdSPIoC8GHYdgO3vDXdJ0n7j8N5HSnohIn5Wer/vSDpV0jPDfcOWLdKmTdIxx2TnpUsHv2bFCqm9XdqwQbr88sHXV66Upk2T7rpLuvbawddvuklqa5PWrJGuv37w9VtvlSZNklavzo6B1q2T9tlHuu46ae3awdc3bszO11wj3X33ntdaWqT167OvL7tMuv/+Pa/vt9+uG5AvWTL4TlRTpuzalGzhwsGrUw89VFq1Kvu6s1N6/vk9r7e379rO4Oyzpe3b97x+9NHSVVdlX59+uvSrX+15/YQTpEsuyb4+6SSpb8D0+tmzpcWLs6+PO06DnHmmdN550quvSiefPPj6/PnZ8fLL0hlnDL5+7rnS3LnStm3SvHmDry9aJM2Zk/0eLVgw+PrFF0szZ2b/bv3bO0iTNVGTtfOdT+qVA34++JuGMdR/H7972df87g2+PvTv3i5XXlnd595wRloI9teSzpb0yoDnrezDu1qTJW3b7fF2Se8b+CLbnZI6JWnvvY8Yh7cFyjdp63R98pS369+ffEJ9r72ROg4wLoYdBLa9XtLXIuLBIa49FBEfqOqN7TMkzYqIfyg9nifpfRHxueG+h0FgNIKL73xSNz88cmvgxeWn1CkNMLpKBoEXDPXhX7JsHDL1SGrb7fGU0nNAQ7v8tOlaMbc9dQygaiMVgI22LywN1kqSbL/D9s2SvjEO7/2IpENsH2z7zZI+IWm4cQegoZw2Y/Kwf+Xz1z/yYqQxgPdKWi6p2/b5kqZL+qKkr0n6ZLVvHBE7bX9O0j2SJki6MSKervbnAvX04vJTdPbZ2dfcFQx5M2wBiIjfSFpQ+vDfIOkXko6KiO3DfU+5ImKdpHXj9fOAFAbOWAHyYtguINsTba+U9HeSZkm6VdJ628fXKxwAoHZG6gJ6VNJ1kj4bETsl3Wu7XdJ1trdGxFl1SQgAqImRCsAHBnb3RES3pGNsf6a2sQAAtTbSGMCwPZsR8a+1iQPkz9FHp04AVIZbQgJV6t+iAMgb7gcAAAVFAQCqdPrp2QHkDV1AQJUG7kwJ5AUtAAAoKAoAABQUBQAACooxAKBKJ5yQOgFQGQoAUKX+WxECeUMXEAAUFAUAqNJJJ2UHkDdJCoDtj9t+2vYbtgfdpxLIk76+7ADyJlUL4ClJH5P0UKL3B4DCSzIIHBHPSpLtFG8PAFAOxgBsd9rust3V29ubOg4ANI2atQBsb5C0/xCXlkXEd8f6cyJilaRVktTR0RHjFA8YN7Nnp04AVKZmBSAiZtbqZwONZPHi1AmAyjR8FxAAoDZSTQP9qO3tko6W9H3b96TIAYyH447LDiBvUs0CukPSHSneGwCQoQsIAAqKAgAABUUBAICCYjtooEpnnpk6AVAZCgBQpfPOS50AqAxdQECVXn01O4C8oQUAVOnkk7Pzxo1JYwBlowUAAAVFAQCAgqIAAEBBUQAAoKAYBAaqNH9+6gRAZSgAQJUoAMgruoCAKr38cnYAeUMLAKjSGWdkZ9YBIG9S3RDmatvP2X7C9h22J6bIAQBFlqoL6D5Jh0fEEZKel7QkUQ4AKKwkBSAi7o2InaWHD0uakiIHABRZIwwCf1rS+uEu2u603WW7q7e3t46xAKC51WwQ2PYGSfsPcWlZRHy39JplknZKumW4nxMRqyStkqSOjo6oQVSgKueemzoBUJmaFYCImDnSddvzJc2WdEJE8MGO3Jo7N3UCoDJJpoHaniXpQkl/ExHspI5c27YtO7e1pc0BlCvVOoB/kbS3pPtsS9LDEXFOoixAVebNy86sA0DeJCkAEfGuFO8LANilEWYBAQASoAAAQEFRAACgoNgMDqjSokWpEwCVoQAAVZozJ3UCoDJ0AQFV2rIlO4C8oQUAVGnBguzMOgDkDS0AACgoCgAAFBQFAAAKigIAAAXFIDBQpYsvTp0AqAwFAKjSzBHvfAE0LrqAgCp1d2cHkDe0AIAqLVyYnVkHgLxJ0gKwfZntJ2x3277X9oEpcgBAkaXqAro6Io6IiHZJd0u6NFEOACisJAUgIn6728N9JXFTeACos2RjALavkPRJSf8r6YOpcgBAUTmiNn98294gaf8hLi2LiO/u9rolkt4SEV8a5ud0SuqUpIMOOui9W7durUVcoGKbNmXnY45JmwMYju3NEdEx6PlaFYCxsn2QpHURcfhor+3o6Iiurq46pAKA5jFcAUg1C+iQ3R6eKum5FDmA8bBp065WAJAnqcYAltueJukNSVslnZMoB1C1pUuzM+sAkDdJCkBEnJ7ifQEAu7AVBAAUFAUAAAqKAgAABcVmcECVVqxInQCoDAUAqFJ7e+oEQGXoAgKqtGFDdgB5QwsAqNLll2dn7gyGvKEFAAAFRQEAgIKiAABAQVEAAKCgGAQGqrRyZeoEQGUoAECVpk1LnQCoDF1AQJXuuis7gLyhBQBU6dprs/OcOWlzAOWiBQAABZW0ANheZDtsT0qZAwCKKFkBsN0m6cOSfp4qAwAUWcoWwDckXSgpEmYAgMJKMghs+1RJPRHxuO3RXtspqVOSDjrooDqkA8pz002pEwCVqVkBsL1B0v5DXFomaamy7p9RRcQqSaskqaOjg9YCGk5bW+oEQGVqVgAiYsjNcW1Pl3SwpP6//qdIetT2kRHx37XKA9TKmjXZee7ctDmActW9CyginpT05/2Pbb8oqSMiXq53FmA8XH99dqYAIG9YBwAABZV8JXBETE2dAQCKiBYAABQUBQAACip5FxCQd7femjoBUBkKAFClSexkhZyiCwio0urV2QHkDQUAqBIFAHnliPzsrmC7V9LWGr7FJEl5XpBG/nTynF0if2q1zv/OiGgd+GSuCkCt2e6KiI7UOSpF/nTynF0if2qp8tMFBAAFRQEAgIKiAOxpVeoAVSJ/OnnOLpE/tST5GQMAgIKiBQAABUUBAICCogAMYPsy20/Y7rZ9r+0DU2caK9tX236ulP8O2xNTZyqH7Y/bftr2G7ZzM6XP9izbW2y/YPui1HnKYftG2y/Zfip1lkrYbrP9oO1nSr8756fONFa232L7J7YfL2X/ct0zMAawJ9tvi4jflr7+R0mHRcQ5iWONie0PS3ogInba/qokRcQ/JY41ZrbfLekNSSslLY6IrsSRRmV7gqTnJX1I0nZJj0g6KyKeSRpsjGx/QNIrkv4jIg5Pnadctg+QdEBEPGr7TyVtlnRaHv79nd0Td9+IeMX2XpJ+KOn8iHi4XhloAQzQ/+Ffsq+k3FTIiLg3InaWHj6s7H7LuRERz0bEltQ5ynSkpBci4mcR8QdJ35F0auJMYxYRD0n6deoclYqIX0bEo6WvfyfpWUmT06Yam8i8Unq4V+mo6+cNBWAItq+wvU3S30q6NHWeCn1a0vrUIQpgsqRtuz3erpx8ADUb21MlzZD047RJxs72BNvdkl6SdF9E1DV7IQuA7Q22nxriOFWSImJZRLRJukXS59Km3dNo2UuvWSZpp7L8DWUs+YFy2X6rpNskLRzQim9oEfF6RLQra60fabuu3XCFvB9ARMwc40tvkbRO0pdqGKcso2W3PV/SbEknRAMO8JTxb58XPZLadns8pfQc6qTUf36bpFsi4vbUeSoRETtsPyhplqS6DcgXsgUwEtuH7PbwVEnPpcpSLtuzJF0o6SMR8WrqPAXxiKRDbB9s+82SPiHpe4kzFUZpIPUGSc9GxNdT5ymH7db+mXq2W5RNJKjr5w2zgAawfZukacpmo2yVdE5E5OIvOtsvSNpb0q9KTz2clxlMkmT7o5L+WVKrpB2SuiPixLSpRmf7ZEkrJE2QdGNEXJE40pjZ/rak45RtR/w/kr4UETckDVUG2++X9J+SnlT2/6wkLY2IdelSjY3tIyR9S9nvzZskrY2Ir9Q1AwUAAIqJLiAAKCgKAAAUFAUAAAqKAgAABUUBAICCogAAZSjtPvlftt9eevxnpcdTbX/K9k9Lx6dSZwVGwzRQoEy2L5T0rojotL1S0ovKdjDtktShbEOvzZLeGxG/SRYUGAUtAKB835B0lO2Fkt4v6RpJJyrbzOvXpQ/9+5Qt6wcaViH3AgKqERGv2b5A0g8kfbj0mF1BkTu0AIDKnCTpl5JydxMVoB8FACiT7XZlG3cdJekLpbtSsSsocodBYKAMpd0nN0m6NCLus/15ZYXg88oGfv+q9NJHlQ0C5/ZuW2h+tACA8nxG0s8j4r7S4+skvVvSdEmXKdse+hFJX+HDH42OFgAAFBQtAAAoKAoAABQUBQAACooCAAAFRQEAgIKiAABAQVEAAKCg/h93cEismSjjlwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCGICOw7TBTO"
      },
      "source": [
        "### Bad Example of Learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KN8QSW9CSZ6X",
        "outputId": "55aa50b4-f626-4b7b-a636-30b0d6573d32"
      },
      "source": [
        "# Too high Learning rate : lr = 10.0\n",
        "init_x = np.array([-3.0, 4.0])\n",
        "lr10, _ = gradient_descent(function_2, init_x, 10.0, 100)\n",
        "print(lr10)\n",
        "\n",
        "# Too low Learning rate  : lr = 1e-10\n",
        "init_x = np.array([-3.0, 4.0])\n",
        "lr1e, _ = gradient_descent(function_2, init_x, 1e-10, 100)\n",
        "print(lr1e)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-2.58983747e+13 -1.29524862e+12]\n",
            "[-2.99999994  3.99999992]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hndlR8BqT09B"
      },
      "source": [
        "learning rate가 너무 큰 경우에는 값이 너무 큰 곳으로 발산해버린다.  \n",
        "반대로 너무 작으면 값의 변화가 거의 변하지 못한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3kk-YTFUUXw"
      },
      "source": [
        "# Gradient in Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xufseqLnUk7s"
      },
      "source": [
        "신경망 학습에서의 기울기를 구해보자.  \n",
        "여기서의 기울기는 가중치 매개변수에 대한 손실 함수의 기울기이다.  \n",
        "\n",
        "\n",
        "예를 들어 형상이 $2 \\times 3$, 가중치가 W, 손실 함수가 L인 신경망을 생각해보자.  \n",
        "경사의 식은 다음과 같다.  \n",
        "\n",
        "$$\n",
        "W = \\begin{pmatrix}\n",
        "w_{11} &w_{12} &w_{13} \\\\\n",
        "w_{21} &w_{22} &w_{23}\n",
        "\\end{pmatrix}\n",
        "$$  \n",
        "\n",
        "$$\n",
        "\\frac{\\partial{L}}{\\partial{W}} =  \\begin{pmatrix}\n",
        "\\frac{\\partial{L}}{\\partial{w_{11}}} &\\frac{\\partial{L}}{\\partial{w_{12}}} &\\frac{\\partial{L}}{\\partial{w_{13}}}\\\\\n",
        "\\frac{\\partial{L}}{\\partial{w_{21}}} &\\frac{\\partial{L}}{\\partial{w_{22}}} &\\frac{\\partial{L}}{\\partial{w_{23}}}\n",
        "\\end{pmatrix}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7Z1AN1GZJB7"
      },
      "source": [
        "## SimpleNet class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KqGM-RWThfZ"
      },
      "source": [
        "class simpleNet:\n",
        "  def __init__(self):\n",
        "    self.W = np.random.randn(2, 3)  # initialize for normal dist\n",
        "  \n",
        "  def predict(self, x):\n",
        "    return np.dot(x, self.W)\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    z = self.predict(x)\n",
        "    y = softmax(z)\n",
        "    loss = cross_entropy_error(y, t)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hrkn9uQqhCGw"
      },
      "source": [
        "### Example of SimpleNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJU1zJq2ZuUj",
        "outputId": "0c5c4694-d6ba-4c29-8cb8-eec5d7f2f4e5"
      },
      "source": [
        "net = simpleNet()\n",
        "print(net.W)  # weight parameter\n",
        "\n",
        "x = np.array([0.6, 0.9])\n",
        "p = net.predict(x)\n",
        "print(p)\n",
        "\n",
        "print(np.argmax(p)) # index of maximum\n",
        "\n",
        "t = np.array([0, 0, 1]) # answer label\n",
        "print(net.loss(x, t))\n",
        "\n",
        "f = lambda w: net.loss(x, t)\n",
        "\n",
        "dW = numerical_gradient(f, net.W)\n",
        "print(dW)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.2262579  -0.34412829 -1.04104513]\n",
            " [-1.16493654  1.53880511  0.76192732]]\n",
            "[-0.91268814  1.17844763  0.06110751]\n",
            "1\n",
            "1.4893828027586582\n",
            "[[ 0.05109825  0.41359452 -0.46469278]\n",
            " [ 0.07664738  0.62039178 -0.69703916]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8VDR466n4Ry"
      },
      "source": [
        "만약 위에서 `numerical_gradient`를 구현한 것을 사용한다면 에러가 난다.  \n",
        "이유는 해당 함수는 1차원 배열의 경우로만 동작한다.  \n",
        "그래서 `common`에 있는 `numerical_gradient`를 가져와야한다.  "
      ]
    }
  ]
}